\section{general}%
\label{sec:general}

to inpect logs in case a container exits:
\begin{verbatim}
	$docker ps -a; docker -log container_name
\end{verbatim}

to attach to a container terminal use:
\begin{verbatim}
	$docker exec -it container_name sh
\end{verbatim}

to validate a dc.yml just run config:
\begin{verbatim}
	$dc config
\end{verbatim}

\section{networking}%
\label{sec:networking}

containers on the same network can communicate with each other and will do so thru the container ports (cf in docker-compose ports: host:docker)

As an example, if we have a mariadb container, and we wish to do a backup, it's convenient to setup a separate container for the procedure, slap it with alpine linux and mysql-client packages, then setup this specific container to live on the same network as the mariadb container, and use the mariadb's name as the host (rather than localhost or outright connecting into the db externally) a further explanation is provided here:
\begin{verbatim}
	https://medium.com/@ricardolsmendes/mysql-mariadb-with-scheduled-backup-jobs-running-in-docker-1956e9892e78
\end{verbatim}

\section{setting up php frameworks}%
\label{sec:setting_up_php_frameworks}

When building Dockerfile for php, make sure all the prerequisites have been also declared. This would be either apt install (deb) or apk install (alpine).
In general the php image can and should live on its own, provided it lives on the same network as the nginx server..

What's more important is that some frameworks will require composer be installed. Either automate it thru the dockerfile or manually docker exec -it into the image and do magic.

Finally, a lot of these frameworks may want to create certain directories that are by default not writeable. So docker exec -it into the image and chmod 767 to make it writeable by the user(docker process).

